#!/usr/bin/env python3
"""
Anna's Archive Wikipedia URL Scraper
è‡ªåŠ¨æŠ“å– Wikipedia é¡µé¢ä¸Š Anna's Archive çš„ URL ä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from urllib.parse import urljoin


def fetch_wikipedia_page(url: str) -> str:
    """è·å– Wikipedia é¡µé¢ HTML å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    return response.text


def extract_urls_from_infobox(html: str) -> list:
    """ä»å³ä¾§ä¿¡æ¯æ¡†ä¸­æå– URL"""
    soup = BeautifulSoup(html, 'html.parser')
    urls = []
    
    # æŸ¥æ‰¾ä¿¡æ¯æ¡† (infobox)
    infobox = soup.find('table', {'class': 'infobox'})
    
    if infobox:
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« URL çš„è¡Œ
        rows = infobox.find_all('tr')
        for row in rows:
            # æŸ¥æ‰¾è¡¨å¤´åŒ…å« "URL" çš„è¡Œ
            th = row.find('th')
            if th and 'URL' in th.get_text():
                # æŸ¥æ‰¾è¯¥è¡Œä¸­çš„æ‰€æœ‰é“¾æ¥
                td = row.find('td')
                if td:
                    links = td.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        if href and href.startswith('http'):
                            urls.append({
                                'url': href,
                                'display_text': text
                            })
    
    return urls


def extract_mirror_sites_from_content(html: str) -> list:
    """ä»æ­£æ–‡å†…å®¹ä¸­æå–é•œåƒç«™ç‚¹ä¿¡æ¯"""
    soup = BeautifulSoup(html, 'html.parser')
    mirrors = []
    
    # æŸ¥æ‰¾åŒ…å«é•œåƒç«™ç‚¹ä¿¡æ¯çš„æ®µè½
    content = soup.find('div', {'id': 'mw-content-text'})
    if content:
        text = content.get_text()
        
        # æŸ¥æ‰¾ .li å’Œ .gl åŸŸåçš„æåŠ
        domain_patterns = [
            r'(\.(?:li|gl|se|gs|org))',
        ]
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨é“¾æ¥
        external_links = content.find_all('a', {'class': 'external'})
        for link in external_links:
            href = link.get('href', '')
            if 'annas-archive' in href or 'anna' in href.lower():
                mirrors.append({
                    'url': href,
                    'source': 'external_link'
                })
    
    return mirrors


def save_to_json(data: dict, filename: str = 'urls.json'):
    """ä¿å­˜æ•°æ®ä¸º JSON æ ¼å¼"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ å·²ä¿å­˜åˆ° {filename}")


def save_to_markdown(data: dict, filename: str = 'urls.md'):
    """ä¿å­˜æ•°æ®ä¸º Markdown æ ¼å¼"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("# Anna's Archive ç½‘å€åˆ—è¡¨\n\n")
        f.write(f"> æœ€åæ›´æ–°æ—¶é—´: {data['last_updated']}\n\n")
        f.write(f"> æ•°æ®æ¥æº: [{data['source']}]({data['source']})\n\n")
        
        f.write("## ä¸»è¦ç½‘å€\n\n")
        f.write("| ç½‘å€ | æ˜¾ç¤ºæ–‡æœ¬ |\n")
        f.write("|------|----------|\n")
        
        for url_info in data['urls']:
            url = url_info.get('url', '')
            text = url_info.get('display_text', '')
            f.write(f"| [{url}]({url}) | {text} |\n")
        
        if data.get('mirror_sites'):
            f.write("\n## é•œåƒç«™ç‚¹\n\n")
            for mirror in data['mirror_sites']:
                url = mirror.get('url', '')
                f.write(f"- [{url}]({url})\n")
        
        f.write("\n---\n\n")
        f.write("## è¯´æ˜\n\n")
        f.write("æ­¤æ–‡ä»¶ç”±è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆï¼Œæ¯å¤©è‡ªåŠ¨ä» Wikipedia è·å–æœ€æ–°ä¿¡æ¯ã€‚\n")
    
    print(f"âœ“ å·²ä¿å­˜åˆ° {filename}")


def main():
    """ä¸»å‡½æ•°"""
    wiki_url = "https://en.wikipedia.org/wiki/Anna%27s_Archive"
    
    print(f"ğŸ”„ æ­£åœ¨æŠ“å–: {wiki_url}")
    
    try:
        # è·å–é¡µé¢å†…å®¹
        html = fetch_wikipedia_page(wiki_url)
        
        # æå– URL
        urls = extract_urls_from_infobox(html)
        mirrors = extract_mirror_sites_from_content(html)
        
        # æ„å»ºæ•°æ®
        data = {
            'last_updated': datetime.now().isoformat(),
            'source': wiki_url,
            'urls': urls,
            'mirror_sites': mirrors
        }
        
        # ä¿å­˜æ–‡ä»¶
        save_to_json(data, 'urls.json')
        save_to_markdown(data, 'urls.md')
        
        print(f"\nâœ… æŠ“å–å®Œæˆ!")
        print(f"   æ‰¾åˆ° {len(urls)} ä¸ªä¸»è¦ URL")
        print(f"   æ‰¾åˆ° {len(mirrors)} ä¸ªé•œåƒç«™ç‚¹")
        
        # æ‰“å°ç»“æœ
        print("\nğŸ“‹ æŠ“å–ç»“æœ:")
        for url_info in urls:
            print(f"   - {url_info['url']} ({url_info.get('display_text', 'N/A')})")
        
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


if __name__ == '__main__':
    main()
